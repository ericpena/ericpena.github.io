<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-162942761-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-162942761-1');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="PCA using Singular Value Decomposition" />
<meta property="og:description" content="PCA Singlular Value Decomposition" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ericpena.github.io/learning/pca-svd.html" /><meta property="article:section" content="learning" />
<meta property="article:published_time" content="2020-07-04T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-07-04T00:00:00+00:00" />


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="PCA using Singular Value Decomposition"/>
<meta name="twitter:description" content="PCA Singlular Value Decomposition"/>


<meta name="generator" content="Hugo 0.101.0" />


    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "PCA using Singular Value Decomposition",
  "url": "https:\/\/ericpena.github.io\/learning\/pca-svd.html",
  "wordCount": "1560",
  "datePublished": "2020-07-04T00:00:00\u002b00:00",
  "dateModified": "2020-07-04T00:00:00\u002b00:00",
  "author": {
    "@type": "Person",
    "name": "Eric Peña"
  },
  "description": "PCA Singlular Value Decomposition"
}
</script>



    <title>PCA using Singular Value Decomposition</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://ericpena.github.io/css/custom.css" rel="stylesheet">
    <link href="https://ericpena.github.io/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container">
            <a class="navbar-brand" href="https://ericpena.github.io/about/eric_pena.html">Eric Peña</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="navbar-nav">

                    <li class="nav-item">
                        <a class="nav-link" href="https://ericpena.github.io">PEÑA NOTES</a>
                    </li>

                    

                    

                    

                    <li class="nav-item">
                        <a class="nav-link" href="https://ericpena.github.io/about/eric_pena.html">About Me</a>
                    </li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true"
                            aria-expanded="false">
                            CONNECT
                        </a>
                        <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                            <a class="dropdown-item" href="https://www.linkedin.com/in/eric-pena/" target="_blank">LinkedIn</a>
                            <a class="dropdown-item" href="https://twitter.com/ericpenax" target="_blank">X</a>
                            
                            <a class="dropdown-item" href="https://github.com/ericpena" target="_blank">GitHub</a>
                            <a class="dropdown-item" href="mailto:pena.eng.physics@gmail.com" target="_blank">Email</a>
                            
                            
                            
                        </div>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                


<article>
  <div class="non-technical_note">
    <header>
      <h1 class="non-technical_note_title">PCA using Singular Value Decomposition</h1>
      <div class="non-technical_note_date">
        <time datetime=" 2020-07-04T00:00:00Z "> 4 July 2020</time>
      </div>
    </header>
    <div class="content">
    <p>In another article, Principal Component Analysis was performed using Eigenvalue Decomposition. In this article we take a different approach: Single Value Decomposition. The general idea is that for any matrix we may perform Single Value Decomposition - this is guaranteed. It is a powerful tool that is used in many fields. On the other hand, the Eigenvalue Decomposition does not always exist. Eigenvalue Decomposition can only be done on square, full-rank, positive semi-definite matricies.</p>
<p>The general formula for Single Value Decomposition:</p>
<p>$$\Sigma = U \Lambda V^*$$</p>
<ul>
<li>
<p>Remember that $V^* = V^T = V^{-1}$ for Real Unitary matrices</p>
</li>
<li>
<p>If we had performed SVD on the covariance of the original data, $X_c$, then we will have essentially performed the same Eigenvalue decomposition as before. Therefore, we will perform SVD on the actual input matrix $X_c$ itself. This matrix need not be square nor linearly independent for SVD to help us here.</p>
</li>
<li>
<p>A general note about <code>np.linalg.svd(...)</code> is that the $V$ that it returns, the rotation matrix of the output space, is actually $V^T$</p>
</li>
</ul>
<h2 id="we-must-start-with-some-preliminary-data-generation">We must start with some preliminary data generation</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">math</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span> <span class="k">as</span> <span class="n">sklearnPCA</span>
</span></span><span class="line"><span class="cl"><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">44</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
</span></span></code></pre></div><p>Here, I generate data according to a Normal Bivariate Distribution described below:</p>
<p>$$X ~ N(\mu, \Sigma)\ \ \ \ \ \ \ \ \ \Sigma = \begin{bmatrix}
1 &amp; 3\\
3 &amp; 10
\end{bmatrix}$$</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;go&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Bivariate Normal Distribution Data&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;$X_1$&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;$X_2$&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img src="pca-svd/pca-svd_6_0.png" alt="png"></p>
<p>Next, I estimate the covariance matrix and compare it with $\Sigma$. The way to do this is listed in the 3 steps below:</p>
<ol>
<li>Remove the means from each data point in $X$: $X_c = (X - \bar X)$</li>
<li>Calculate: $X_c^T X_c$</li>
<li>Divide by the number of degrees of freedom: $(N-1)$</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">mx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Xc</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">mx</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span> <span class="o">=</span> <span class="p">(</span><span class="n">Xc</span><span class="o">.</span><span class="n">T</span><span class="nd">@Xc</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Xc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">C</span>
</span></span></code></pre></div><pre><code>array([[ 0.98999791,  3.01642667],
       [ 3.01642667, 10.21045548]])
</code></pre>
<p>We can see that the computed covariance matrix is quite similar to the original covariance. The basic idea is that <code>cov</code> $\approx$ <code>C</code>.</p>
<p>Now, I will use the <code>svd</code> function from NumPy:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">U</span><span class="p">,</span> <span class="n">E</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;U: Rotation of the input space</span><span class="se">\n</span><span class="si">{</span><span class="n">U</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;E: Scaling in the output space</span><span class="se">\n</span><span class="si">{</span><span class="n">E</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;VT: Rotation of the output space</span><span class="se">\n</span><span class="si">{</span><span class="n">VT</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>U: Rotation of the input space
[[-0.28565762 -0.95833174]
 [-0.95833174  0.28565762]]

E: Scaling in the output space
[11.10958594  0.09086745]

VT: Rotation of the output space
[[-0.28565762 -0.95833174]
 [-0.95833174  0.28565762]]
</code></pre>
<p>There are a few importnat points I should mention:</p>
<ul>
<li>
<p>$U$ and $V$ are both orthonormal matrices (assuming $\Sigma$ is real)</p>
</li>
<li>
<p>The columns of $V$ and $U$ are the Eigenvectors of $X_c^T X_c$ and $X_c X_c^T$, respectively.</p>
</li>
<li>
<p>We can show what the data looks like in the new orthogonal directions</p>
</li>
<li>
<p>Using the formula is $Z_k = X V_k$</p>
</li>
</ul>
<p>Now, let&rsquo;s plot the data in the new bases:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">SZ</span> <span class="o">=</span> <span class="n">Xc</span> <span class="o">@</span> <span class="n">VT</span><span class="o">.</span><span class="n">T</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">SZ</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">SZ</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;go&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Single Value Composition Projection Into New Bases&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Eigenvector 1 [PC: 1]&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Eigenvector 2 [PC: 2]&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img src="pca-svd/pca-svd_13_0.png" alt="png"></p>
<p>Next we may see if this transformation preserves the variance of the original data</p>
<p>The general formula for Single Value Decomposition:
$$\Sigma = U \Lambda V^{*}$$</p>
<p>Since $\Lambda$ is the diagonal matrix from decomposing $X_c$, the trace of this matrix (sum of $\Lambda_{ii}$) is the following:
$$Tr(\Lambda) = \frac{d_1^2}{N-1} + \frac{d_2^2}{N-1} + \cdots + \frac{d_p^2}{N-1} = \sigma^2_{Z_1} + \sigma^2_{Z_2} + \cdots + \sigma^2_{Z_p}  = Var(Z)$$</p>
<p>But in this case, the $\frac{d_i}{N - 1} = \lambda_i$ since SVD and Eigenvalue Decomposition is the same when done on the covariance matrix.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">xVariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">sVariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">E</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Variance in newly found orthogonal space: </span><span class="si">{</span><span class="n">sVariance</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Variance in original orthogonal space: </span><span class="si">{</span><span class="n">xVariance</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Variance in newly found orthogonal space: 11.200453390218739
Variance in original orthogonal space: 11.178052483438304
</code></pre>
<p>These values are very close to one another therefore variance (information) is preserved.</p>
<h2 id="more-features">More Features!</h2>
<p>We typically have more features that we&rsquo;re dealing with so let&rsquo;s introduce this into what we&rsquo;re doing.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">N</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl"><span class="n">meanNP</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span>
</span></span></code></pre></div><p>Start with randomly creating a covariance matrix</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">covNP</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="n">covNP</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">covNP</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">covNP</span> <span class="o">=</span> <span class="p">(</span><span class="n">covNP</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">covNP</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">covNP</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">covNP</span>
</span></span></code></pre></div><pre><code>array([[1., 2., 2., 5., 3., 2., 3., 2., 4., 4.],
       [2., 1., 2., 5., 4., 1., 2., 1., 5., 5.],
       [2., 2., 1., 4., 3., 1., 1., 1., 3., 3.],
       [5., 5., 4., 1., 7., 5., 4., 3., 8., 7.],
       [3., 4., 3., 7., 1., 4., 3., 3., 5., 5.],
       [2., 1., 1., 5., 4., 1., 2., 2., 3., 2.],
       [3., 2., 1., 4., 3., 2., 1., 1., 4., 2.],
       [2., 1., 1., 3., 3., 2., 1., 1., 1., 2.],
       [4., 5., 3., 8., 5., 3., 4., 1., 1., 6.],
       [4., 5., 3., 7., 5., 2., 2., 2., 6., 1.]])
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">min_eig</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">covNP</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">min_eig</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">     <span class="n">covNP</span> <span class="o">-=</span> <span class="mi">10</span><span class="o">*</span><span class="n">min_eig</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="o">*</span><span class="n">covNP</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span></span></code></pre></div><p>Create $X_{N \times p}$ input matrix with $N = 5000$ and $p = 10$ | Sample from the same Bivariate Normal Distribution</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">XNP</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">meanNP</span><span class="p">,</span> <span class="n">covNP</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">show_pairplot</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">XNP_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">XNP</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;X&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">p</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>
</span></span><span class="line"><span class="cl">    <span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">XNP_df</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">diag_kind</span> <span class="o">=</span> <span class="s1">&#39;kde&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">height</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">palette</span><span class="o">=</span><span class="s2">&#34;husl&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">             <span class="n">kind</span><span class="o">=</span><span class="s2">&#34;reg&#34;</span><span class="p">,</span> <span class="n">plot_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;line_kws&#39;</span><span class="p">:{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span><span class="s1">&#39;red&#39;</span><span class="p">}},</span>
</span></span><span class="line"><span class="cl">             <span class="n">markers</span><span class="o">=</span><span class="s2">&#34;+&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">show_pairplot</span><span class="p">()</span>
</span></span></code></pre></div><p><img src="pca-svd/pca-svd_25_0.png" alt="png"></p>
<p>Next we will:</p>
<ol>
<li>Subtract the mean from the input matrix</li>
<li>Calculate the covariance matrix</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">mxNP</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">XNP</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">XcNP</span> <span class="o">=</span> <span class="n">XNP</span> <span class="o">-</span> <span class="n">mxNP</span>
</span></span><span class="line"><span class="cl"><span class="n">CNP</span> <span class="o">=</span> <span class="n">XcNP</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">XcNP</span> <span class="o">/</span> <span class="p">(</span><span class="n">XcNP</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></div><p>Now we may show the calculated covariance matrix:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">CNP</span><span class="p">);</span>
</span></span></code></pre></div><p><img src="pca-svd/pca-svd_29_0.png" alt="png"></p>
<p>We can take a look at the differences in covariances between the one used for data generation and the one from simulated data.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">covNP</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Covariance Matrix Used For Data Generation&#39;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;VARIABLE INDICIES&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;VARIABLE INDICIES&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">CNP</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Calculated Covariance Matrix From Simulated Data&#39;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;VARIABLE INDICIES&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;VARIABLE INDICIES&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img src="pca-svd/pca-svd_31_0.png" alt="png"></p>
<p>Here, I perform <strong>SVD</strong> on this problem:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">UNP</span><span class="p">,</span> <span class="n">ENP</span><span class="p">,</span> <span class="n">VTNP</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">XcNP</span><span class="p">,</span> <span class="n">full_matrices</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;U: Rotation of the input space</span><span class="se">\n</span><span class="si">{</span><span class="n">UNP</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;E: Scaling in the output space</span><span class="se">\n</span><span class="si">{</span><span class="n">ENP</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;VT: Rotation of the output space</span><span class="se">\n</span><span class="si">{</span><span class="n">VTNP</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>U: Rotation of the input space
[[ 0.00063402 -0.00223972 -0.00512473 ... -0.01283197 -0.00249885
   0.00729883]
 [ 0.00398954 -0.00015302  0.01754178 ...  0.02282558 -0.00582044
  -0.01374769]
 [-0.0113012  -0.02463393  0.00080511 ...  0.00957205 -0.00641299
  -0.0096895 ]
 ...
 [-0.01955479 -0.02007181  0.01777133 ...  0.02102236 -0.00992698
   0.01587713]
 [-0.01690699 -0.00056427 -0.0182975  ...  0.01105485 -0.01346963
  -0.01055169]
 [ 0.01078462  0.01053594  0.01148998 ... -0.00896266 -0.01786641
   0.00072838]]

E: Scaling in the output space
[740.25549159 650.23894313 645.07816186 630.69002383 620.78564169
 614.66865467 609.89320315 606.82283464 599.25632365 582.99093422]

VT: Rotation of the output space
[[-0.29282796 -0.30797533 -0.28041494 -0.43695612 -0.43217361 -0.23051754
  -0.14681501 -0.16859798 -0.34464154 -0.37572558]
 [ 0.22877855  0.08402528  0.48977653 -0.05238006 -0.04269272 -0.52141728
   0.25666469 -0.59697826 -0.03962057  0.02114617]
 [-0.02011072 -0.34199333  0.733593   -0.06685807  0.00687695  0.34513918
  -0.30369872  0.13985565 -0.32820467 -0.03644537]
 [ 0.11779768  0.51624749  0.12470635 -0.14611295 -0.32952993 -0.34994377
  -0.5445956   0.33456606  0.01191737  0.20736469]
 [-0.22851051  0.11055657 -0.11981725  0.23791751 -0.0332837   0.2896755
  -0.53419567 -0.66916026  0.07310094  0.2027199 ]
 [ 0.83224791 -0.3533297  -0.24941654  0.1600689  -0.07765862 -0.00781993
  -0.27364501 -0.0552323  -0.04264587 -0.09406375]
 [-0.05189883 -0.30030924 -0.15682762 -0.03306807 -0.04685503 -0.13316574
   0.12933948  0.04138274 -0.36434896  0.84280043]
 [-0.08953451 -0.25994462  0.15540834  0.2883993  -0.73282976  0.07156255
   0.17669037  0.0684583   0.48055385  0.08993209]
 [-0.27447973 -0.44545874  0.04204989  0.20817596  0.37818413 -0.54242154
  -0.33505051  0.12740806  0.33807826 -0.03300058]
 [-0.12933967  0.14616797 -0.02638978  0.75543034 -0.11562563 -0.17987633
   0.05742537  0.1011913  -0.53539761 -0.21123923]]
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">UNP</span><span class="p">,</span> <span class="n">ENP</span><span class="p">,</span> <span class="n">VTNP</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">Xc</span><span class="p">,</span> <span class="n">full_matrices</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># print(f&#39;U: Rotation of the input space\n{UNP}\n&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;E: Scaling in the output space</span><span class="se">\n</span><span class="si">{</span><span class="n">ENP</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;VT: Rotation of the output space</span><span class="se">\n</span><span class="si">{</span><span class="n">VTNP</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>E: Scaling in the output space
[74.4559157   6.73371059]

VT: Rotation of the output space
[[ 0.28565762  0.95833174]
 [ 0.95833174 -0.28565762]]
</code></pre>
<p>It appears that since there are many more dimensions, the variance explain is spread throughout the different orthogonal components. The question becomes, is the amount of varation in all these components equal to the amount of variation from the original dataset. To answer this we will sum up all the diagonal entries from the $\Lambda$ matrix like previously discussed. The important formula we will use is the following:</p>
<p>$$Tr(\Lambda) = \frac{d_1^2}{N} + \frac{d_2^2}{N} + \cdots + \frac{d_p^2}{N} = \sigma^2_{Z_1} + \sigma^2_{Z_2} + \cdots + \sigma^2_{Z_p}  = Var(Z)$$</p>
<h4 id="amount-of-variation-from-svd-components">Amount of Variation From SVD Components:</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">varNP</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">ENP</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="amount-of-variation-from-the-original-simulated-data">Amount of Variation From The Original Simulated Data</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">varXcNP</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">XcNP</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">)])</span>
</span></span></code></pre></div><h4 id="comparison-of-the-two-values-above">Comparison Of The Two Values Above</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Variance in newly found orthogonal space: </span><span class="si">{</span><span class="n">varNP</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Variance in original orthogonal space: </span><span class="si">{</span><span class="n">varXcNP</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Variance in newly found orthogonal space: 797.4134455431736
Variance in original orthogonal space: 797.413445543174
</code></pre>
<ul>
<li>
<p>As we can see above, the amount of variation explained is still preserved even when switching to a new orthogonal basis</p>
</li>
<li>
<p>Now let&rsquo;s look into how much variation is explained by each variable</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">tvar</span> <span class="o">=</span> <span class="n">varNP</span>
</span></span><span class="line"><span class="cl"><span class="n">vlist</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">ENP</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">cumvar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">vlist</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">vlist</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">cumvar</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vlist</span> <span class="o">/</span> <span class="n">tvar</span><span class="p">))</span>
</span></span></code></pre></div><pre><code>[109.59563857  84.56213663  83.22516698  79.55398123  77.07496258
  75.56351101  74.39394385  73.64679053  71.82162829  67.97568588]
[109.59563857 194.1577752  277.38294218 356.93692341 434.011886
 509.575397   583.96934085 657.61613138 729.43775967 797.41344554]
1.0
</code></pre>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="s1">&#39;seaborn-whitegrid&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">vlist</span> <span class="o">/</span> <span class="n">tvar</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;individual explained variance&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="p">),</span> <span class="n">cumvar</span> <span class="o">/</span> <span class="n">tvar</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="s1">&#39;mid&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;cumulative explained variance&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Explained variance ratio&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Principal components&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span></code></pre></div><p><img src="pca-svd/pca-svd_46_0.png" alt="png"></p>
<p>We can even check if this decomposition was successful. Let&rsquo;s reconstruct our original data by multiplying all three matrices we obtain from the decomposition.</p>
<p>$$U \Lambda V^T \rightarrow X$$</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">ENPmatrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">ENP</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">XNPconstructed</span> <span class="o">=</span> <span class="n">UNP</span> <span class="o">@</span> <span class="n">ENPmatrix</span> <span class="o">@</span> <span class="n">VTNP</span>
</span></span><span class="line"><span class="cl"><span class="n">frobenius</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">XcNP</span> <span class="o">-</span> <span class="n">XNPconstructed</span><span class="p">,</span> <span class="s1">&#39;fro&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Frobenius Norm value between constructed Xc from the decomposition and the original dataset: </span><span class="si">{</span><span class="n">frobenius</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</span></span></code></pre></div><pre><code>Frobenius Norm value between constructed Xc from the decomposition and the original dataset: 4.281900349435277e-12
</code></pre>
<p>A Frobenius Norm value this low shows that the decomposition was performed correctly</p>
<p>$$||A||_F = \sqrt{Tr\left(AA^H\right)}$$</p>
<!-- raw HTML omitted -->
<h2 id="we-did-it">We did it!</h2>

  </div>
  
      </div>
      
  </article>



            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">Copyright &copy; Eric Peña, <time datetime="2024">2024</time>. All 29 notes and articles are available on <a href="https://github.com/ericpena/">GitHub</a>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>