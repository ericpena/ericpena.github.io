<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning - Eric Pe√±a</title>
    <link>//ericpena.github.io/machine_learning/basics/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Jul 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="//ericpena.github.io/machine_learning/basics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>K-Means Clustering From Scratch</title>
      <link>//ericpena.github.io/machine_learning/basics/k-means.html</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>//ericpena.github.io/machine_learning/basics/k-means.html</guid>
      <description>Introduction Imagine we have a scattering of points whose labels or group assignments are completely unknown to us and moreover, we haven&amp;rsquo;t a clue as to the number of groups. Unsupervised clustering helps us in such a situation and allows us to assign a cluster to each point. These clusters are defined by their centroids (centers). The K-Means algorithm will iteratively update these centroids to find the best location for them.</description>
    </item>
    
    <item>
      <title>PCA using Singular Value Decomposition</title>
      <link>//ericpena.github.io/machine_learning/basics/pca-svd.html</link>
      <pubDate>Sat, 04 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>//ericpena.github.io/machine_learning/basics/pca-svd.html</guid>
      <description>In another article, Principal Component Analysis was performed using Eigenvalue Decomposition. In this article we take a different approach: Single Value Decomposition. The general idea is that for any matrix we may perform Single Value Decomposition - this is guaranteed. It is a powerful tool that is used in many fields. On the other hand, the Eigenvalue Decomposition does not always exist. Eigenvalue Decomposition can only be done on square, full-rank, positive semi-definite matricies.</description>
    </item>
    
    <item>
      <title>PCA using Eigenvalue Decomposition</title>
      <link>//ericpena.github.io/machine_learning/basics/pca-evd.html</link>
      <pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>//ericpena.github.io/machine_learning/basics/pca-evd.html</guid>
      <description>In this article, I go into how we can perform Principal Component Analysis (PCA) using the method of Eigenvalue Decomposition (EVD). Generally, PCA is done by peforming a change of basis on the data, typically by utilizing eigenvectors that find the principal directions of the data. Another important thing to know is that the Eigenvalue Decomposition does not always exist. Eigenvalue Decomposition can only be done on square, full-rank, positive semi-definite matricies.</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>//ericpena.github.io/machine_learning/basics/linear-regression.html</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      
      <guid>//ericpena.github.io/machine_learning/basics/linear-regression.html</guid>
      <description>Mathematical Foundations The linear model is often the first model we learn fitting data. Given a vector of inputs $X^T = (X_1, X_2, \ldots, X_p)$, we can predict the output $Y$ with the following model:
$$\hat Y = \hat \beta_0 + \sum_{j=1}^p X_j \hat \beta_j$$
Many times, it&amp;rsquo;s convenient to include the vector $\textbf{1}$ in $\textbf{X}$ and include the $\hat \beta_0$ in the vector $\hat \beta$ so we may represent this linear model in vector form as an inner product:</description>
    </item>
    
  </channel>
</rss>