<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-162942761-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-162942761-1');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Linear Regression" />
<meta property="og:description" content="Linear Regression using Least Squares" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ericpena.github.io/machine_learning/basics/linear-regression.html" />
<meta property="article:published_time" content="2020-05-29T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-05-29T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Linear Regression"/>
<meta name="twitter:description" content="Linear Regression using Least Squares"/>
<meta name="generator" content="Hugo 0.68.3" />

    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Linear Regression",
  "url": "https:\/\/ericpena.github.io\/machine_learning\/basics\/linear-regression.html",
  "wordCount": "1046",
  "datePublished": "2020-05-29T00:00:00\x2b00:00",
  "dateModified": "2020-05-29T00:00:00\x2b00:00",
  "author": {
    "@type": "Person",
    "name": "Eric Pe√±a"
  },
  "description": "Linear Regression using Least Squares"
}
</script> 

    <title>Linear Regression</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://ericpena.github.io/css/custom.css" rel="stylesheet"> 
    <link href="https://ericpena.github.io/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">

    
    <link href="" rel="alternate" type="application/rss+xml" title="Eric Pe√±a All Notes And Articles" /> 
    
    <link href="https://ericpena.github.io//articles/index.xml" rel="alternate" type="application/rss+xml" title="Eric Pe√±a Articles" />

    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container">
            <a class="navbar-brand" href="https://ericpena.github.io/">Eric Pe√±a</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="navbar-nav">

                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true"
                            aria-expanded="false">
                            Pe√±a Notes
                        </a>
                        <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                            <a class="dropdown-item" href="https://ericpena.github.io/#physics">Physics</a>
                            <a class="dropdown-item" href="https://ericpena.github.io/#complex_systems">Complex Systems</a>
                            <a class="dropdown-item" href="https://ericpena.github.io/#mathematics">Mathematics</a>
                            <a class="dropdown-item" href="https://ericpena.github.io/#statistics">Statistics</a>
                            <a class="dropdown-item" href="https://ericpena.github.io/#network_science">Network Science</a>
                            <a class="dropdown-item" href="https://ericpena.github.io/#machine_learning">Machine Learning</a>
                            <a class="dropdown-item" href="https://ericpena.github.io/#python">Python</a>
                            <a class="dropdown-item" href="https://ericpena.github.io/#mathematica">Mathematica</a>
                            <a class="dropdown-item" href="https://ericpena.github.io/#julia">Julia</a>
                            <a class="dropdown-item" href="https://ericpena.github.io/#r_programming">R Programming</a>
                        </div>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="https://ericpena.github.io/#projects">Projects</a>
                    </li>

                    <li class="nav-item">
                        <a class="nav-link" href="https://ericpena.github.io/#articles">Articles</a>
                    </li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true"
                            aria-expanded="false">
                            About
                        </a>
                        <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                            <a class="dropdown-item" href="https://ericpena.github.io/about/eric_pena.html">About E.Pe√±a</a>
                            <a class="dropdown-item" href="https://github.com/ericpena" target="_blank">GitHub</a>
                            <a class="dropdown-item" href="https://twitter.com/ericpenaphysics" target="_blank">Twitter</a>
                            <a class="dropdown-item" href="https://www.linkedin.com/in/eric-pena/" target="_blank">LinkedIn</a>
                            
                            
                            
                        </div>
                    </li>
                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
      
    <h1 class="technical_note_title">Linear Regression</h1>
    <div class="technical_note_date">
      <time datetime=" 2020-05-29T00:00:00Z "> 29 May 2020</time>
    </div>
  </header>
  <div class="content">
      
  <h1 id="mathematical-foundations">Mathematical Foundations</h1>
<p>The linear model is often the first model we learn fitting data. Given a vector of inputs $X^T = (X_1, X_2, \ldots, X_p)$, we can predict the output $Y$ with the following model:</p>
<p>$$\hat Y = \hat \beta_0 + \sum_{j=1}^p X_j \hat \beta_j$$</p>
<p>Many times, it&rsquo;s convenient to include the vector $\textbf{1}$ in $\textbf{X}$ and include the $\hat \beta_0$ in the vector $\hat \beta$ so we may represent this linear model in vector form as an inner product:</p>
<p>$$\hat Y = X^T \hat \beta$$</p>
<p>According to this notation, $X^T$ is a row vector and $X$ is a column vector. To achieve our fit by means of Least Squares, we choose the coefficients $\beta$ to minimize the <em>residual sum of squares:</em></p>
<p>$$RSS(\beta) = \sum_{i=1}^N (y_i - x_i^T \beta)^2$$</p>
<p>In matrix form:</p>
<p>$$
\begin{equation}
RSS(\beta) = (\mathbf{y} - \mathbf{X} \beta)^T (\mathbf{y} - \mathbf{X} \beta)
\end{equation}
$$</p>
<p>where $\textbf{X}$ is an $N \times p$ matrix with each row being an input vector and $\textbf{y}$ is an $N$-vector of the outputs. If we differentiate with respect to $\beta$, then we obtain the following equation:</p>
<p>$$\textbf{X}^T (\textbf{y} - \textbf{X} \beta) = 0$$</p>
<p>So long as $\textbf{X}^T \textbf{X}$ is <em>nonsingular</em>, then the unique solution is given by the following equation:</p>
<p>$$\hat \beta = (\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \textbf{y}$$</p>
<p>where $(\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \rightarrow$ <em>pseudo-inverse</em> or <em>Moore‚ÄìPenrose inverse</em></p>
<p>That is to say the prediction value of the $i^{th}$ input $x_i$ is $\hat y_i = \hat y(x_i) = x_i^T \hat \beta$</p>
<h1 id="practice">Practice</h1>
<p>Say we generated our own data using the following rules:</p>
<p>$$y = 5 + 4x_1 + 3x_2 ‚àí 2x_3 + \epsilon$$</p>
<p>with $\epsilon \sim \mathcal{N}(0, 0.5^2)$, $x_1 \sim \mathcal{U}(0, 4)$, $x_2 \sim \mathcal{U}(1, 4)$, $x_3 \sim \mathcal{N}(3, 0.4^2)$, $ùëÅ = 20$</p>
<p>$\mathcal{N} \rightarrow$ Normal Distribution and $\mathcal{U} \rightarrow$ Uniform Distribution</p>
<h1 id="preamble">Preamble</h1>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Constants</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">44</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">3</span>
</code></pre></div><h1 id="generate-input-data">Generate Input Data</h1>
<p>$$x_1 \sim \mathcal{U}(0, 4)$$
$$x_2 \sim \mathcal{U}(1, 4)$$
$$x_3 \sim \mathcal{N}(3, 0.4^2)$$</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X</span>
</code></pre></div><pre><code>array([[1.        , 3.33936859, 1.31438831, 2.69975411],
       [1.        , 1.43724335, 2.82771514, 3.52654293],
       [1.        , 1.5751182 , 2.22721783, 3.74351348],
       [1.        , 3.8421049 , 2.36986333, 3.03503519],
       [1.        , 1.71060608, 1.3403911 , 2.52663634],
       [1.        , 2.54732806, 1.41743876, 3.24217837],
       [1.        , 1.83481628, 3.62158956, 3.33026594],
       [1.        , 3.45069734, 1.44654419, 2.51607458],
       [1.        , 2.25179983, 1.47746579, 2.91483235],
       [1.        , 3.17829966, 3.97246804, 2.43540034],
       [1.        , 3.2200688 , 2.13224622, 2.71767879],
       [1.        , 2.84438274, 1.21752344, 3.01007254],
       [1.        , 3.53044633, 3.17817346, 3.20816395],
       [1.        , 2.78971967, 3.79030841, 3.33008896],
       [1.        , 3.526691  , 1.28491595, 2.79435513],
       [1.        , 0.43589402, 1.46120338, 1.77052447],
       [1.        , 3.93713613, 1.81468046, 2.84193637],
       [1.        , 1.22969024, 2.26616556, 2.68184033],
       [1.        , 1.32398556, 2.70422612, 3.16740439],
       [1.        , 3.60697592, 1.0475734 , 2.90586647]])
</code></pre>
<h1 id="generate-output-data">Generate Output Data</h1>
<p>$$y = 5 + 4x_1 + 3x_2 ‚àí 2x_3 + \epsilon$$
$$\epsilon \sim \mathcal{N}(0, 0.5^2)$$</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">y</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="n">x1</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x2</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">x3</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">3</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">y</span>
</code></pre></div><pre><code>array([17.11718468, 12.61417894, 11.49941045, 22.09382466, 11.33962392,
       13.67994053, 16.56791356, 18.15543755, 12.3698036 , 24.91013347,
       19.3981462 , 14.00029869, 22.23957398, 20.55036089, 18.17014326,
        7.59544376, 20.78941751, 11.25433856, 12.3054156 , 15.84934396])
</code></pre>
<h1 id="fit-model-find-hat-beta">Fit Model (Find $\hat \beta$)</h1>
<p>$$\hat \beta = (\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \textbf{y}$$</p>
<p>$$RSS(\beta) = \sum_{i=1}^N (y_i - x_i^T \beta)^2$$</p>
<p>Also, the unbiased estimate for $\sigma^2$:</p>
<p>$$\hat \sigma^2 = \frac{1}{N - p - 1} \sum_{i = 1}^N (y_i - \hat y_i)^2$$</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">beta_hat</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">X</span><span class="p">)</span> <span class="err">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="err">@</span> <span class="n">y</span>
<span class="n">RSS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="err">@</span> <span class="n">beta_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span>
<span class="n">sigma2_hat</span> <span class="o">=</span>  <span class="n">RSS</span><span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="n">p</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div><p>$\hat \beta$ = <code>[ 4.66154677,  3.96733899,  2.93094973, -1.7340953 ]</code></p>
<p>$RSS$ = <code>3.5247737039908507</code></p>
<p>$\hat \sigma^2$ = <code>0.22029835649942817</code></p>
<h1 id="generate-testing-data">Generate Testing Data</h1>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">sample_size</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sample_size</span><span class="p">)]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div><h1 id="find-test-values-and-prediction-values">Find Test Values and Prediction Values</h1>
<p>$$y_{test} = \textbf{X} \beta$$
$$y_{pred} = \textbf{X} \hat \beta$$</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_test</span> <span class="o">=</span> <span class="n">X_test</span> <span class="err">@</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">X_test</span> <span class="err">@</span> <span class="n">beta_hat</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;b.-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="s1">&#39;r.-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Prediction Values vs True Values&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Sample Number&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Value Output&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img src="linear-regression/linear-regression_19_0.png" alt="png"></p>
<h1 id="using-scikit-learn-library">Using Scikit-Learn Library</h1>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Import Package</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Fit Model</span>
<span class="n">lm</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;beta_sklearn = {lm.coef_}&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;coefficient_of_determination = {lm.score(X, y)}&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>beta_sklearn = [ 4.66154677  3.96733899  2.93094973 -1.7340953 ]
coefficient_of_determination = 0.9912772830386247
</code></pre>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">y_pred_sklearn</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;b.-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_pred_sklearn</span><span class="p">,</span> <span class="s1">&#39;g.-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;sklearn: Prediction Values vs True Values&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Sample Number&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Value Output&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><p><img src="linear-regression/linear-regression_23_0.png" alt="png"></p>
<h1 id="confidence-interval-for-hat-beta">Confidence Interval for $\hat \beta$</h1>
<h3 id="variance-of-hat-beta">Variance of $\hat \beta$</h3>
<p>We learned above that the estimator $\hat \beta$ can written as:</p>
<p>$$\hat \beta = (X^T X)^{-1} X^T y$$</p>
<p>It is worth mentioning that this is an unbiased estimator: $E\left[\hat \beta\right] = \beta$. From this we can write:</p>
<p>$$Var\left[\hat \beta\right] = E\left[\hat \beta^2\right] - E\left[\hat \beta\right] \cdot E\left[\hat \beta^T\right]$$</p>
<p>$$Var\left[\hat \beta\right] = E\left[\left(\left(X^T X\right)^{-1} X^T y\right)^2\right] - \hat \beta^2$$</p>
<p>The underlying true regression is:</p>
<p>$$y = \textbf{X} \beta + \epsilon$$</p>
<p>Plug the information about $y$ into the expression for $Var\left[\hat \beta\right]$:</p>
<p>$$E\left[\left(\left(X^T X\right)^{-1} X^T y\right)^2\right] - \hat \beta^2 = E\left[\left(\left(X^T X\right)^{-1}X^T\left(X \beta + \epsilon\right)\right)^2\right] - \beta^2$$</p>
<p>$$E\left[\left(\left(X^T X\right)^{-1}X^T X \beta + (X^T X)^{-1} X^T \epsilon\right)^2\right] - \beta^2$$</p>
<p>where $\left(X^T X\right)^{-1}X^T X \rightarrow 1$</p>
<p>$$E\left[\left(\beta + (X^T X)^{-1} X^T \epsilon\right)^2\right] - \beta^2$$</p>
<p>$$\beta^2 + E\left[\left((X^T X)^{-1} X^T \epsilon\right)^2\right] - \beta^2$$</p>
<p>Since $E\left[\epsilon\right] = 0$ and the $\beta^2$ cancels out:</p>
<p>$$Var\left[ \hat \beta \right] = \left(\left(X^T X\right)^{-1} X^T\right)^2 E\left[\epsilon^2\right]$$</p>
<p>We assume that $E\left[ \epsilon^2 \right] = \sigma^2$. Therefore, the variance of $\hat \beta$ becomes:</p>
<p>$$Var\left[ \hat \beta \right] = \sigma^2 (X^T X)^{-1}$$</p>
<p>The variance of $\hat \beta$ is independent of the true coefficients $\beta$.</p>
<h3 id="confidence-interval">Confidence Interval</h3>
<p>$$\hat \beta_j \pm t_{\frac{\alpha}{2}, N-p-1} \sqrt{C_{jj} \sigma^2}$$</p>
<p>where $C_{jj}$ are the diagonal elements of the covariance matrix $\rightarrow (X^T X)^{-1}$</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Variance of Beta</span>
<span class="n">var_beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">sigma2_hat</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="err">@</span> <span class="n">X</span> <span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;variance_beta = </span><span class="se">\n</span><span class="s1"> {var_beta}&#39;</span><span class="p">)</span>

<span class="c1"># Use 2.12 for the t statistic (alpha = 0.05)</span>
<span class="n">beta_interval</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">beta_hat</span> <span class="o">-</span> <span class="mf">2.12</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_beta</span><span class="p">),</span> <span class="n">beta_hat</span> <span class="o">+</span> <span class="mf">2.12</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var_beta</span><span class="p">)]</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;beta_interval = </span><span class="se">\n</span><span class="s1"> {beta_interval}&#39;</span><span class="p">)</span>
</code></pre></div><pre><code>variance_beta = 
 [0.5786609  0.01103045 0.01555414 0.0683109 ]
beta_interval = 
 [[ 3.04886778  6.27422577]
 [ 3.74468396  4.18999402]
 [ 2.66655126  3.19534819]
 [-2.28818602 -1.18000458]]
</code></pre>
<h2 id="references">References</h2>
<ul>
<li><code>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol. 1, no. 10. New York: Springer series in statistics, 2001.</code></li>
<li><code>Majte. How to derive variance-covariance matrix of coefficients in linear regression. Cross Validated. [https://stats.stackexchange.com/q/86104] (version: 2019-01-08)</code></li>
</ul>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Thank You For Reading!</h4>
          <p>All material is saved on GitHub. Please <a href='https://github.com/ericpena/pena-notes/issues/new'>submit a suggested change</a> and include the note's URL.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">Copyright &copy; Eric Pe√±a, <time datetime="2020">2020</time>. All 15 notes and articles are available on <a href="https://github.com/ericpena/">GitHub</a>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>