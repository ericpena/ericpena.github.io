<!DOCTYPE HTML>


<script type="text/javascript">
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$']],
            displayMath: [['$$', '$$']]
        }
    };
</script>
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
</script>

<html lang="en">
	<head>
	  <title>Tensor Intro</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="referrer" content="origin">

	

    <meta name="description" content="Senior Data Scientist
Understanding Complexity Through Data and Statistics">
    
    <meta name="generator" content="Hugo 0.138.0">

    
<link rel="stylesheet" href="../../css/main.min.6470f359d96cadb62114d84a859a08e047c933e85d36b945421da8260b62381977f4f70390c7eab26f30d96e167059bdd0e86e799c562d5d4eba32b5a9d4713c.css" integrity="sha512-ZHDzWdlsrbYhFNhKhZoI4EfJM&#43;hdNrlFQh2oJgtiOBl39PcDkMfqsm8w2W4WcFm90OhueZxWLV1OujK1qdRxPA==">


<noscript><link rel="stylesheet" href="../../css/noscript.min.e6f1ba19697eecfddfbf83ff7181b98181998f163d7005f6ae923451556bf85bef357f43dffe1522b92c1efab7fb38441f479e39b7a03e4313a8ef12b0b01f65.css" integrity="sha512-5vG6GWl&#43;7P3fv4P/cYG5gYGZjxY9cAX2rpI0UVVr&#43;FvvNX9D3/4VIrksHvq3&#43;zhEH0eeObegPkMTqO8SsLAfZQ=="></noscript>





    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Tensor Intro">
  <meta name="twitter:description" content="This article provides an introduction to tensors, exploring their geometric and mathematical definitions, transformation rules, notation, and applications, with detailed explanations of vectors, covectors, and coordinate system changes">
      <meta name="twitter:site" content="@ericpenax">

    <meta property="og:url" content="https://&lt;username&gt;.github.io/posts/tensor/">
  <meta property="og:site_name" content="Eric Peña">
  <meta property="og:title" content="Tensor Intro">
  <meta property="og:description" content="This article provides an introduction to tensors, exploring their geometric and mathematical definitions, transformation rules, notation, and applications, with detailed explanations of vectors, covectors, and coordinate system changes">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2019-04-20T00:00:00+00:00">
    <meta property="article:modified_time" content="2019-04-20T00:00:00+00:00">
    <meta property="article:tag" content="Mathematics">
    <meta property="article:tag" content="Physics">

    
	</head>
	<body class="landing is-preload">

		
			<div id="page-wrapper">

				
          <header id="header">
            <h1><a href="https://%3cusername%3e.github.io/">Eric Peña</a></h1>
						<nav id="nav">
							<ul>
								<li class="special">
                  <a href="#menu" class="menuToggle" aria-label='Menu'><span>Menu</span></a>
									<div id="menu">
										<ul>
				              
				              <li><a href="../../">Home</a></li>
				              
				              <li><a href="../../about/">About Me</a></li>
				              
				              <li><a href="../../posts/">Posts</a></li>
				              
										</ul>
									</div>
								</li>
							</ul>
						</nav>
					</header>

<article id="main">
  <header >
    <h2>Tensor Intro</h2>
    
	</header>
	<section class="wrapper style5">
		<div class="inner">
      <p><img src="../../images/tensor-intro/Vector-1-Form.svg"></p>
<p>Motivation:</p>
<ul>
<li>General Relativity</li>
<li>Inertia Tensor</li>
<li>Stress Tensor</li>
</ul>
<p>It took me a whole week figure out what a tensor actually is. You will find many definitions and some are only partially correct. Let&rsquo;s walk through the different explanations and learn how to think about them.</p>
<ol>
<li><strong>(Array Definition)</strong> Tensor = Multi-dimensional array of numbers (scalars (rank 0), vectors (rank 1), matricies (rank 2), etc.). This is true in a sense but there is a truer geometrical meaning behind the concept of a tensor.</li>
<li><strong>(Coordinate Definition)</strong> Tensor = an object that is invariant under a change of coordinates and has <em>components</em> that change in a special, predictable way under a change of coordinates. This is also true but let&rsquo;s dive even deeper to learn how a tensor allows this behavior to take place.</li>
<li><strong>(Abstract Definition)</strong> Tensor = a collection of vectors and covectors combined together using the tensor product</li>
</ol>
<p>The important thing to keep in mind is that vectors exist independently of their components and their components depend on the coordinate system used to define vectors.</p>
<h1 id="changing-coordinate-systems">Changing Coordinate Systems</h1>
<p>Let&rsquo;s go from an old basis to a new basis</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<h3 id="forward-transformation">Forward Transformation</h3>
<p>Suppose:
$$\tilde{\vec{e_1}} = 2\vec{e_1}+1\vec{e_2}$$
$$\tilde{\vec{e_2}} = -\frac{1}{2}\vec{e_1}+\frac{1}{4}\vec{e_2}$$
$$F = \begin{pmatrix}2 &amp; 1\\ -\frac{1}{2} &amp; \frac{1}{4}\end{pmatrix}$$
where $F$ is the Forward Transformation Matrix.</p>
<h3 id="backward-transformation">Backward Transformation</h3>
<p>This would make the backward transformation:
$$\vec{e_1} = \frac{1}{4}\tilde{\vec{e_1}}+(-1)\tilde{\vec{e_2}}$$
$$\vec{e_2} = \frac{1}{2}\tilde{\vec{e_1}}+2\tilde{\vec{e_2}}$$
$$B = \begin{pmatrix}\frac{1}{4} &amp; -1\\ \frac{1}{2} &amp; 2\end{pmatrix}$$</p>
<h3 id="forward-and-backward-transformation">Forward and Backward Transformation</h3>
<p>$$F \cdot B = \begin{pmatrix}2 &amp; 1\\ -\frac{1}{2} &amp; \frac{1}{4}\end{pmatrix} \begin{pmatrix}\frac{1}{4} &amp; -1\\ \frac{1}{2} &amp; 2\end{pmatrix} = \begin{pmatrix}1 &amp; 0\\ 0 &amp; 1\end{pmatrix}$$</p>
<p>In general:
$$F \cdot B = \delta_{ij}$$
$$F = B^{-1}; B = F^{-1}$$
$$\tilde{\vec{e_i}} = \sum_{j=1}^{n} F_{ij} \vec{e_j}$$
$$\vec{e_i} = \sum_{j=1}^{n} B_{ij} \tilde{\vec{e_j}}$$</p>
<h1 id="vectors">Vectors</h1>
<p>Vectors are the first example of a tensor. Vectors are invariant but their components are <em>NOT</em> invariant. It is also important to know that not all vectors are geometrical Euclidean vectors. Some vectors that are harder to visualize.</p>
<p>The transformation rules for vectors behave in an opposite way compared to the basis vectors {$\vec{e_1}, \vec{e_2}$}</p>
<p>$$\vec{v} = \sum_{j=1}^{n}v_j \vec{e_j} = \sum_{i=1}^{n} \tilde{v_i} \tilde{\vec{e_i}}$$</p>
<p>$$\vec{v} = \sum_{j=1}^{n} v_j \vec{e_j} = \sum_{j=1}^{n} v_j \left( \sum_{j=1}^{n} B_{ij} \tilde{\vec{e_j}} \right) = \sum_{i=1}^{n} \left( \sum_{j=1}^{n} B_{ij} v_j \right) \tilde{\vec{e_i}}$$</p>
<p>So this shows that:
$$\tilde{v_i} = \sum_{j=1}^{n} B_{ij} v_j$$
Which basically means that if you want to define the vector is the new basis, we have to use the backwards transformation matrix. This will take some getting used to since it is opposite of the unit basis. For this reason, vectors are said to contravary and are even called <strong>contravariant vectors</strong>.</p>
<!-- raw HTML omitted -->
<table>
  <thead>
      <tr>
          <th style="text-align: center">Basis</th>
          <th style="text-align: center">Vectors</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">$$\tilde{\vec{e_i}} = \sum_{j=1}^{n} F_{ij} \vec{e_j}$$</td>
          <td style="text-align: center">$$\tilde{v_i} = \sum_{j=1}^{n} B_{ij} v_j$$</td>
      </tr>
      <tr>
          <td style="text-align: center">$$\vec{e_i} = \sum_{j=1}^{n} B_{ij} \tilde{\vec{e_j}}$$</td>
          <td style="text-align: center">$$v_i = \sum_{j=1}^{n} F_{ij} \tilde{v_j}$$</td>
      </tr>
  </tbody>
</table>
<!-- raw HTML omitted -->
<ul>
<li>Upper indicies represent contravariant components</li>
<li>Lower indicies represent covariant components</li>
<li>Einstein notation uses the upper and lower indicies and also drops the $\sum$ symbol</li>
<li>When a covector $\alpha_j$ is acting on a vector $v^j$, it can be written as $\alpha_j v^j$ and is assumed to be the sum:
$$\alpha_j v^j = \sum_{j=1}^n \alpha_j v^j = \alpha_1 v^1 + \alpha_2 v^2 + \alpha_3 v^3 + &hellip; + \alpha_n v^n$$</li>
<li>Using this new notation convention, the formulas in the table above can be written as:</li>
</ul>
<!-- raw HTML omitted -->
<table>
  <thead>
      <tr>
          <th style="text-align: center">Basis</th>
          <th style="text-align: center">Vectors</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">$$\tilde{\vec{e_i}} = F_i^j \vec{e_j}$$</td>
          <td style="text-align: center">$$\tilde{v^i} = B_j^i v^j$$</td>
      </tr>
      <tr>
          <td style="text-align: center">$$\vec{e_i} = B_i^j \tilde{\vec{e_j}}$$</td>
          <td style="text-align: center">$$v^i = F_j^i \tilde{v^j}$$</td>
      </tr>
  </tbody>
</table>
<!-- raw HTML omitted -->
<h1 id="covectors">Covectors</h1>
<p>Covectors may be harder to visualize since it differs from the arrow Euclidean vectors that is often used in physics. Here are a couple initial notes about covectors:</p>
<ul>
<li>Covectors can be thought of as row vectors but clarification is needed for this. Row vectors in this sense are not necessarily column vectors flipped on their side. This is only true when using an orthonormal basis. If the basis is not orthonormal, it is more apparent how row vectors are different from column vectors.</li>
<li>It is better to think of covectors as functions that act on vectors and map them to real numbers:</li>
</ul>
<p>$$\alpha: V \rightarrow \mathbb{R}$$</p>
<ul>
<li>When we visualize vectors, we think of them as Euclidean vectors with components in a system of coordinates. A covector can be visualized as directed stacks of lines (or surfaces). Below are a few images that can help visualize covectors:</li>
</ul>
<p><img src="../../images/tensor-intro/covector1.png">
<img src="../../images/tensor-intro/covector0.png">
<img src="../../images/tensor-intro/covector2.png"></p>
<p>The inner product is a mechanism used to combine a covector and vector. The output is a number that represents the number of surfaces of the covector that are pierced by the vector.</p>
<h3 id="covector-rules-to-keep-in-mind">Covector Rules To Keep In Mind:</h3>
<p>Covector acting on a vector:
$$\alpha(\vec{v}) = \alpha_1 v^1 + \alpha_2 v^2 + \alpha_3 v^3 + &hellip; + \alpha_n v^n = \sum_{j=1}^n \alpha_j v^j$$</p>
<p>Properties of Linearity:
$$\alpha(\vec{v} + \vec{w}) = \alpha(\vec{v}) + \alpha(\vec{w})$$</p>
<p>$$\alpha(n \vec{v}) = n \alpha(\vec{v})$$</p>
<p>$$(\beta + \gamma)(\vec{v}) = \beta(\vec{v}) + \gamma(\vec{v})$$</p>
<p>Vector Spaces:</p>
<ul>
<li>When scaling and combining vectors, a vector space $V$ is spanned</li>
<li>Covectors can also be scaled with scalars and combined using addition and multiplication. The vector space spanned by covectors is called the &ldquo;Dual Vector Space&rdquo;, $V^*$.</li>
</ul>
<h1 id="linear-maps">Linear Maps</h1>
<p>&hellip;to be continued</p>

		</div>
	</section>
</article>
				
					<footer id="footer">
						<ul class="icons">
              
              <li><a href="https://www.linkedin.com/in/eric-pena" class="icon brands fa-linkedin" target="_blank" rel="noopener noreferrer"><span class="label">LinkedIn</span></a></li>
              
              
              <li><a href="https://twitter.com/ericpenax" class="icon brands fa-twitter" target="_blank" rel="noopener noreferrer"><span class="label">Twitter</span></a></li>
              
              
              <li><a href="https://mstdn.science/@ericpena" class="icon brands fa-mastodon" target="_blank" rel="noopener noreferrer"><span class="label">Mastodon</span></a></li>
              
              
              <li><a href="https://github.com/ericpena" class="icon brands fa-github" target="_blank" rel="noopener noreferrer"><span class="label">GitHub</span></a></li>
              
              
              
              
              <li><a href="https://instagram.com/ericpena" class="icon brands fa-instagram" target="_blank" rel="noopener noreferrer"><span class="label">Instagram</span></a></li>
              
              
              
              <li><a href="mailto:eric.pena@binghamton.edu" class="icon solid fa-envelope" target="_blank" rel="noopener noreferrer"><span class="label">Email</span></a></li>
              
              

						</ul>
						<ul class="copyright">
              <li>&copy; 2025 Eric Peña</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</footer>

			</div>

      









<script src="../../js/bundle.min.935254271ae3006602cb92b38ba70062a462cefc8d3aa575338369d256bb8422a69fc18f64450e74b7e6c240b20a252f522f3b9f323294bdf9ed466f5fb28ee9.js" integrity="sha512-k1JUJxrjAGYCy5Kzi6cAYqRizvyNOqV1M4Np0la7hCKmn8GPZEUOdLfmwkCyCiUvUi87nzIylL357UZvX7KO6Q=="></script>


	</body>
</html>

