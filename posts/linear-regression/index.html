<!DOCTYPE HTML>


<script type="text/javascript">
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$']],
            displayMath: [['$$', '$$']]
        }
    };
</script>
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
</script>

<html lang="en">
	<head>
	  <title>Linear Regression</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="referrer" content="origin">

	

    <meta name="description" content="Senior Data Scientist
Understanding Complexity Through Data and Statistics">
    
    <meta name="generator" content="Hugo 0.138.0">

    
<link rel="stylesheet" href="../../css/main.min.6470f359d96cadb62114d84a859a08e047c933e85d36b945421da8260b62381977f4f70390c7eab26f30d96e167059bdd0e86e799c562d5d4eba32b5a9d4713c.css" integrity="sha512-ZHDzWdlsrbYhFNhKhZoI4EfJM&#43;hdNrlFQh2oJgtiOBl39PcDkMfqsm8w2W4WcFm90OhueZxWLV1OujK1qdRxPA==">


<noscript><link rel="stylesheet" href="../../css/noscript.min.e6f1ba19697eecfddfbf83ff7181b98181998f163d7005f6ae923451556bf85bef357f43dffe1522b92c1efab7fb38441f479e39b7a03e4313a8ef12b0b01f65.css" integrity="sha512-5vG6GWl&#43;7P3fv4P/cYG5gYGZjxY9cAX2rpI0UVVr&#43;FvvNX9D3/4VIrksHvq3&#43;zhEH0eeObegPkMTqO8SsLAfZQ=="></noscript>





    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Linear Regression">
  <meta name="twitter:description" content="This article explains the mathematical foundations, practical implementation, and evaluation of linear regression, including confidence intervals for coefficients and a comparison between manual and Scikit-Learn implementations">
      <meta name="twitter:site" content="@ericpenax">

    <meta property="og:url" content="https://ericpena.github.io/posts/linear-regression/">
  <meta property="og:site_name" content="Eric Pe√±a">
  <meta property="og:title" content="Linear Regression">
  <meta property="og:description" content="This article explains the mathematical foundations, practical implementation, and evaluation of linear regression, including confidence intervals for coefficients and a comparison between manual and Scikit-Learn implementations">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2020-05-29T00:00:00+00:00">
    <meta property="article:modified_time" content="2020-05-29T00:00:00+00:00">
    <meta property="article:tag" content="Statistics">
    <meta property="article:tag" content="Programming">
    <meta property="article:tag" content="Data Science">

    
	</head>
	<body class="landing is-preload">

		
			<div id="page-wrapper">

				
          <header id="header">
            <h1><a href="https://ericpena.github.io/">Eric Pe√±a</a></h1>
						<nav id="nav">
							<ul>
								<li class="special">
                  <a href="#menu" class="menuToggle" aria-label='Menu'><span>Menu</span></a>
									<div id="menu">
										<ul>
				              
				              <li><a href="../../">Home</a></li>
				              
				              <li><a href="../../about/">About Me</a></li>
				              
				              <li><a href="../../posts/">Posts</a></li>
				              
										</ul>
									</div>
								</li>
							</ul>
						</nav>
					</header>

<article id="main">
  <header >
    <h2>Linear Regression</h2>
    
	</header>
	<section class="wrapper style5">
		<div class="inner">
      <h2 id="mathematical-foundations">Mathematical Foundations</h2>
<p>The linear model is often the first model we learn fitting data. Given a vector of inputs $X^T = (X_1, X_2, \ldots, X_p)$, we can predict the output $Y$ with the following model:</p>
<p>$$\hat Y = \hat \beta_0 + \sum_{j=1}^p X_j \hat \beta_j$$</p>
<p>Many times, it&rsquo;s convenient to include the vector $\textbf{1}$ in $\textbf{X}$ and include the $\hat \beta_0$ in the vector $\hat \beta$ so we may represent this linear model in vector form as an inner product:</p>
<p>$$\hat Y = X^T \hat \beta$$</p>
<p>According to this notation, $X^T$ is a row vector and $X$ is a column vector. To achieve our fit by means of Least Squares, we choose the coefficients $\beta$ to minimize the <em>residual sum of squares:</em></p>
<p>$$RSS(\beta) = \sum_{i=1}^N (y_i - x_i^T \beta)^2$$</p>
<p>In matrix form:</p>
<p>$$
\begin{equation}
RSS(\beta) = (\mathbf{y} - \mathbf{X} \beta)^T (\mathbf{y} - \mathbf{X} \beta)
\end{equation}
$$</p>
<p>where $\textbf{X}$ is an $N \times p$ matrix with each row being an input vector and $\textbf{y}$ is an $N$-vector of the outputs. If we differentiate with respect to $\beta$, then we obtain the following equation:</p>
<p>$$\textbf{X}^T (\textbf{y} - \textbf{X} \beta) = 0$$</p>
<p>So long as $\textbf{X}^T \textbf{X}$ is <em>nonsingular</em>, then the unique solution is given by the following equation:</p>
<p>$$\hat \beta = (\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \textbf{y}$$</p>
<p>where $(\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \rightarrow$ <em>pseudo-inverse</em> or <em>Moore‚ÄìPenrose inverse</em></p>
<p>That is to say the prediction value of the $i^{th}$ input $x_i$ is $\hat y_i = \hat y(x_i) = x_i^T \hat \beta$</p>
<h2 id="practice">Practice</h2>
<p>Say we generated our own data using the following rules:</p>
<p>$$y = 5 + 4x_1 + 3x_2 ‚àí 2x_3 + \epsilon$$</p>
<p>with $\epsilon \sim \mathcal{N}(0, 0.5^2)$, $x_1 \sim \mathcal{U}(0, 4)$, $x_2 \sim \mathcal{U}(1, 4)$, $x_3 \sim \mathcal{N}(3, 0.4^2)$, $ùëÅ = 20$</p>
<p>$\mathcal{N} \rightarrow$ Normal Distribution and $\mathcal{U} \rightarrow$ Uniform Distribution</p>
<h2 id="preamble">Preamble</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> matplotlib <span style="color:#f92672">import</span> pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Constants</span>
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">44</span>)
</span></span><span style="display:flex;"><span>N <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>p <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</span></span></code></pre></div><h2 id="generate-input-data">Generate Input Data</h2>
<p>$$x_1 \sim \mathcal{U}(0, 4)$$
$$x_2 \sim \mathcal{U}(1, 4)$$
$$x_3 \sim \mathcal{N}(3, 0.4^2)$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1</span>, np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>), np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">4</span>), np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">3</span>, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>)] <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(N)]
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(X)
</span></span><span style="display:flex;"><span>X
</span></span></code></pre></div><pre><code>array([[1.        , 3.33936859, 1.31438831, 2.69975411],
       [1.        , 1.43724335, 2.82771514, 3.52654293],
       [1.        , 1.5751182 , 2.22721783, 3.74351348],
       [1.        , 3.8421049 , 2.36986333, 3.03503519],
       [1.        , 1.71060608, 1.3403911 , 2.52663634],
       [1.        , 2.54732806, 1.41743876, 3.24217837],
       [1.        , 1.83481628, 3.62158956, 3.33026594],
       [1.        , 3.45069734, 1.44654419, 2.51607458],
       [1.        , 2.25179983, 1.47746579, 2.91483235],
       [1.        , 3.17829966, 3.97246804, 2.43540034],
       [1.        , 3.2200688 , 2.13224622, 2.71767879],
       [1.        , 2.84438274, 1.21752344, 3.01007254],
       [1.        , 3.53044633, 3.17817346, 3.20816395],
       [1.        , 2.78971967, 3.79030841, 3.33008896],
       [1.        , 3.526691  , 1.28491595, 2.79435513],
       [1.        , 0.43589402, 1.46120338, 1.77052447],
       [1.        , 3.93713613, 1.81468046, 2.84193637],
       [1.        , 1.22969024, 2.26616556, 2.68184033],
       [1.        , 1.32398556, 2.70422612, 3.16740439],
       [1.        , 3.60697592, 1.0475734 , 2.90586647]])
</code></pre>
<h2 id="generate-output-data">Generate Output Data</h2>
<p>$$y = 5 + 4x_1 + 3x_2 ‚àí 2x_3 + \epsilon$$
$$\epsilon \sim \mathcal{N}(0, 0.5^2)$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">y</span>(x1, x2, x3):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>x1 <span style="color:#f92672">+</span> <span style="color:#ae81ff">3</span><span style="color:#f92672">*</span>x2 <span style="color:#f92672">-</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>x3 <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>,scale<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> [y(X[i][<span style="color:#ae81ff">1</span>], X[i][<span style="color:#ae81ff">2</span>], X[i][<span style="color:#ae81ff">3</span>]) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(N)]
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(y)
</span></span><span style="display:flex;"><span>y
</span></span></code></pre></div><pre><code>array([17.11718468, 12.61417894, 11.49941045, 22.09382466, 11.33962392,
       13.67994053, 16.56791356, 18.15543755, 12.3698036 , 24.91013347,
       19.3981462 , 14.00029869, 22.23957398, 20.55036089, 18.17014326,
        7.59544376, 20.78941751, 11.25433856, 12.3054156 , 15.84934396])
</code></pre>
<h2 id="fit-model-find-hat-beta">Fit Model (Find $\hat \beta$)</h2>
<p>$$\hat \beta = (\textbf{X}^T \textbf{X})^{-1} \textbf{X}^T \textbf{y}$$</p>
<p>$$RSS(\beta) = \sum_{i=1}^N (y_i - x_i^T \beta)^2$$</p>
<p>Also, the unbiased estimate for $\sigma^2$:</p>
<p>$$\hat \sigma^2 = \frac{1}{N - p - 1} \sum_{i = 1}^N (y_i - \hat y_i)^2$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>beta_hat <span style="color:#f92672">=</span> (np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>inv(X<span style="color:#f92672">.</span>T <span style="color:#f92672">@</span> X) <span style="color:#f92672">@</span> X<span style="color:#f92672">.</span>T) <span style="color:#f92672">@</span> y
</span></span><span style="display:flex;"><span>RSS <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum( (y <span style="color:#f92672">-</span> X <span style="color:#f92672">@</span> beta_hat)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span> )
</span></span><span style="display:flex;"><span>sigma2_hat <span style="color:#f92672">=</span>  RSS<span style="color:#f92672">/</span> (N <span style="color:#f92672">-</span> p <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>$\hat \beta$ = <code>[ 4.66154677,  3.96733899,  2.93094973, -1.7340953 ]</code></p>
<p>$RSS$ = <code>3.5247737039908507</code></p>
<p>$\hat \sigma^2$ = <code>0.22029835649942817</code></p>
<h2 id="generate-testing-data">Generate Testing Data</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sample_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>X_test <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1</span>, np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>), np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">4</span>), np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">3</span>, scale<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>)] <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(sample_size)]
</span></span><span style="display:flex;"><span>X_test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(X_test)
</span></span></code></pre></div><h2 id="find-test-values-and-prediction-values">Find Test Values and Prediction Values</h2>
<p>$$y_{test} = \textbf{X} \beta$$
$$y_{pred} = \textbf{X} \hat \beta$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>y_test <span style="color:#f92672">=</span> X_test <span style="color:#f92672">@</span> [<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>y_pred <span style="color:#f92672">=</span> X_test <span style="color:#f92672">@</span> beta_hat
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">15</span>,<span style="color:#ae81ff">10</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter([i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(y_test))], y_test, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;True Value&#39;</span>, s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter([i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(y_pred))], y_pred, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Predicted Value&#39;</span>, s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;blue&#39;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> v <span style="color:#f92672">in</span> [i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(y_pred))]:
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>axvline(x<span style="color:#f92672">=</span>v, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Prediction Values vs True Values&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Sample Number &#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Y Value Output&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xticks(size<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>yticks(size<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend(fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img alt="png" src="../../images/linear-regression/linear-regression_19_0.png"></p>
<h2 id="using-scikit-learn-library">Using Scikit-Learn Library</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Import Package</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LinearRegression
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Fit Model</span>
</span></span><span style="display:flex;"><span>lm <span style="color:#f92672">=</span> LinearRegression(fit_intercept<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)<span style="color:#f92672">.</span>fit(X, y)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;beta_sklearn = </span><span style="color:#e6db74">{</span>lm<span style="color:#f92672">.</span>coef_<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;coefficient_of_determination = </span><span style="color:#e6db74">{</span>lm<span style="color:#f92672">.</span>score(X, y)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span></code></pre></div><pre><code>beta_sklearn = [ 4.66154677  3.96733899  2.93094973 -1.7340953 ]
coefficient_of_determination = 0.9912772830386247
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>y_pred_sklearn <span style="color:#f92672">=</span> lm<span style="color:#f92672">.</span>predict(X_test)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize <span style="color:#f92672">=</span> (<span style="color:#ae81ff">15</span>,<span style="color:#ae81ff">10</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter([i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(y_test))], y_test, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;True Value&#39;</span>, s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;red&#39;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter([i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(y_pred))], y_pred_sklearn, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Predicted Value&#39;</span>, s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;darkgreen&#39;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> v <span style="color:#f92672">in</span> [i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(y_pred))]:
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>axvline(x<span style="color:#f92672">=</span>v, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Prediction Values vs True Values&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Sample Number &#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Y Value Output&#34;</span>, fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xticks(size<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>yticks(size<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend(fontsize<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img alt="png" src="../../images/linear-regression/linear-regression_23_0.png"></p>
<h2 id="confidence-interval-for-beta">Confidence Interval for Beta</h2>
<h3 id="variance-of-beta">Variance of Beta</h3>
<p>We learned above that the estimator $\hat \beta$ can written as:</p>
<p>$$\hat \beta = (X^T X)^{-1} X^T y$$</p>
<p>It is worth mentioning that this is an unbiased estimator: $E\left[\hat \beta\right] = \beta$. From this we can write:</p>
<p>$$Var\left[\hat \beta\right] = E\left[\hat \beta^2\right] - E\left[\hat \beta\right] \cdot E\left[\hat \beta^T\right]$$</p>
<p>$$Var\left[\hat \beta\right] = E\left[\left(\left(X^T X\right)^{-1} X^T y\right)^2\right] - \hat \beta^2$$</p>
<p>The underlying true regression is:</p>
<p>$$y = \textbf{X} \beta + \epsilon$$</p>
<p>Plug the information about $y$ into the expression for $Var\left[\hat \beta\right]$:</p>
<p>$$E\left[\left(\left(X^T X\right)^{-1} X^T y\right)^2\right] - \hat \beta^2 = E\left[\left(\left(X^T X\right)^{-1}X^T\left(X \beta + \epsilon\right)\right)^2\right] - \beta^2$$</p>
<p>$$E\left[\left(\left(X^T X\right)^{-1}X^T X \beta + (X^T X)^{-1} X^T \epsilon\right)^2\right] - \beta^2$$</p>
<p>where $\left(X^T X\right)^{-1}X^T X \rightarrow 1$</p>
<p>$$E\left[\left(\beta + (X^T X)^{-1} X^T \epsilon\right)^2\right] - \beta^2$$</p>
<p>$$\beta^2 + E\left[\left((X^T X)^{-1} X^T \epsilon\right)^2\right] - \beta^2$$</p>
<p>Since $E\left[\epsilon\right] = 0$ and the $\beta^2$ cancels out:</p>
<p>$$Var\left[ \hat \beta \right] = \left(\left(X^T X\right)^{-1} X^T\right)^2 E\left[\epsilon^2\right]$$</p>
<p>We assume that $E\left[ \epsilon^2 \right] = \sigma^2$. Therefore, the variance of $\hat \beta$ becomes:</p>
<p>$$Var\left[ \hat \beta \right] = \sigma^2 (X^T X)^{-1}$$</p>
<p>The variance of $\hat \beta$ is independent of the true coefficients $\beta$.</p>
<h3 id="confidence-interval">Confidence Interval</h3>
<p>$$\hat \beta_j \pm t_{\frac{\alpha}{2}, N-p-1} \sqrt{C_{jj} \sigma^2}$$</p>
<p>where $C_{jj}$ are the diagonal elements of the covariance matrix $\rightarrow (X^T X)^{-1}$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Variance of Beta</span>
</span></span><span style="display:flex;"><span>var_beta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>diag(sigma2_hat <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>inv(X<span style="color:#f92672">.</span>T <span style="color:#f92672">@</span> X ))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;variance_beta = </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> </span><span style="color:#e6db74">{</span>var_beta<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Use 2.12 for the t statistic (alpha = 0.05)</span>
</span></span><span style="display:flex;"><span>beta_interval <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>c_[beta_hat <span style="color:#f92672">-</span> <span style="color:#ae81ff">2.12</span> <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(var_beta), beta_hat <span style="color:#f92672">+</span> <span style="color:#ae81ff">2.12</span> <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(var_beta)]
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;beta_interval = </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> </span><span style="color:#e6db74">{</span>beta_interval<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span></code></pre></div><pre><code>variance_beta = 
 [0.5786609  0.01103045 0.01555414 0.0683109 ]
beta_interval = 
 [[ 3.04886778  6.27422577]
 [ 3.74468396  4.18999402]
 [ 2.66655126  3.19534819]
 [-2.28818602 -1.18000458]]
</code></pre>
<h2 id="references">References</h2>
<ol>
<li>Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning. Vol. 1, no. 10. New York: Springer series in statistics, 2001.</li>
<li>Majte. How to derive variance-covariance matrix of coefficients in linear regression. Cross Validated. [https://stats.stackexchange.com/q/86104](version: 2019-01-08)</li>
</ol>

		</div>
	</section>
</article>
				
					<footer id="footer">
						<ul class="icons">
              
              <li><a href="https://www.linkedin.com/in/eric-pena" class="icon brands fa-linkedin" target="_blank" rel="noopener noreferrer"><span class="label">LinkedIn</span></a></li>
              
              
              <li><a href="https://twitter.com/ericpenax" class="icon brands fa-twitter" target="_blank" rel="noopener noreferrer"><span class="label">Twitter</span></a></li>
              
              
              <li><a href="https://mstdn.science/@ericpena" class="icon brands fa-mastodon" target="_blank" rel="noopener noreferrer"><span class="label">Mastodon</span></a></li>
              
              
              <li><a href="https://github.com/ericpena" class="icon brands fa-github" target="_blank" rel="noopener noreferrer"><span class="label">GitHub</span></a></li>
              
              
              
              
              <li><a href="https://instagram.com/ericpena" class="icon brands fa-instagram" target="_blank" rel="noopener noreferrer"><span class="label">Instagram</span></a></li>
              
              
              
              <li><a href="mailto:eric.pena@binghamton.edu" class="icon solid fa-envelope" target="_blank" rel="noopener noreferrer"><span class="label">Email</span></a></li>
              
              

						</ul>
						<ul class="copyright">
              <li>&copy; 2025 Eric Pe√±a</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</footer>

			</div>

      









<script src="../../js/bundle.min.935254271ae3006602cb92b38ba70062a462cefc8d3aa575338369d256bb8422a69fc18f64450e74b7e6c240b20a252f522f3b9f323294bdf9ed466f5fb28ee9.js" integrity="sha512-k1JUJxrjAGYCy5Kzi6cAYqRizvyNOqV1M4Np0la7hCKmn8GPZEUOdLfmwkCyCiUvUi87nzIylL357UZvX7KO6Q=="></script>


	</body>
</html>

