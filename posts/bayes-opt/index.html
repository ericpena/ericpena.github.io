<!DOCTYPE HTML>


<script type="text/javascript">
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$']],
            displayMath: [['$$', '$$']]
        }
    };
</script>
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
</script>

<html lang="en">
	<head>
	  <title>Bayesian Optimization</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="referrer" content="origin">

	

    <meta name="description" content="Senior Data Scientist
Understanding Complexity Through Data and Statistics">
    
    <meta name="generator" content="Hugo 0.138.0">

    
<link rel="stylesheet" href="../../css/main.min.6470f359d96cadb62114d84a859a08e047c933e85d36b945421da8260b62381977f4f70390c7eab26f30d96e167059bdd0e86e799c562d5d4eba32b5a9d4713c.css" integrity="sha512-ZHDzWdlsrbYhFNhKhZoI4EfJM&#43;hdNrlFQh2oJgtiOBl39PcDkMfqsm8w2W4WcFm90OhueZxWLV1OujK1qdRxPA==">


<noscript><link rel="stylesheet" href="../../css/noscript.min.e6f1ba19697eecfddfbf83ff7181b98181998f163d7005f6ae923451556bf85bef357f43dffe1522b92c1efab7fb38441f479e39b7a03e4313a8ef12b0b01f65.css" integrity="sha512-5vG6GWl&#43;7P3fv4P/cYG5gYGZjxY9cAX2rpI0UVVr&#43;FvvNX9D3/4VIrksHvq3&#43;zhEH0eeObegPkMTqO8SsLAfZQ=="></noscript>





    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Bayesian Optimization">
  <meta name="twitter:description" content="This article provides a step-by-step guide to implementing Bayesian optimization in Python, including designing the algorithm from scratch using NumPy and SciPy, applying it with Python libraries like scikit-optimize, and visualizing the process for optimizing a noisy objective function">
      <meta name="twitter:site" content="@ericpenax">

    <meta property="og:url" content="https://&lt;username&gt;.github.io/posts/bayes-opt/">
  <meta property="og:site_name" content="Eric Peña">
  <meta property="og:title" content="Bayesian Optimization">
  <meta property="og:description" content="This article provides a step-by-step guide to implementing Bayesian optimization in Python, including designing the algorithm from scratch using NumPy and SciPy, applying it with Python libraries like scikit-optimize, and visualizing the process for optimizing a noisy objective function">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2020-04-14T00:00:00+00:00">
    <meta property="article:modified_time" content="2020-04-14T00:00:00+00:00">
    <meta property="article:tag" content="Bayesian">
    <meta property="article:tag" content="Optimization">
    <meta property="article:tag" content="Programming">

    
	</head>
	<body class="landing is-preload">

		
			<div id="page-wrapper">

				
          <header id="header">
            <h1><a href="https://%3cusername%3e.github.io/">Eric Peña</a></h1>
						<nav id="nav">
							<ul>
								<li class="special">
                  <a href="#menu" class="menuToggle" aria-label='Menu'><span>Menu</span></a>
									<div id="menu">
										<ul>
				              
				              <li><a href="../../">Home</a></li>
				              
				              <li><a href="../../about/">About Me</a></li>
				              
				              <li><a href="../../posts/">Posts</a></li>
				              
										</ul>
									</div>
								</li>
							</ul>
						</nav>
					</header>

<article id="main">
  <header >
    <h2>Bayesian Optimization</h2>
    
	</header>
	<section class="wrapper style5">
		<div class="inner">
      <h1 id="bayesian-optimization">Bayesian Optimization</h1>
<p>Below is a walk-through how to perform Bayesian Optimization in Python. This code follows work of Martin Krasser in order to optimize the following objective function:</p>
<p>$$f(x) = 2 \sin{(4 x)} \cos{(x)}$$
$$\text{where } (1 &lt; x &lt; 4)$$</p>
<p>The first section goes through designing a Bayesian Optimization algorithm using Numpy and SciPy. The second section goes into how we can take advantage of a Python package to optimize our function. The is one of the cleanest explanations of Bayesian optimization I&rsquo;ve come across so I found it helpful to go through this procedure myself. Bayesian optimization is also often used to perform hyperparameter optimization.</p>
<h1 id="preamble">Preamble</h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.gaussian_process <span style="color:#f92672">import</span> GaussianProcessRegressor
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.gaussian_process.kernels <span style="color:#f92672">import</span> ConstantKernel, Matern
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.optimize <span style="color:#f92672">import</span> minimize
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.stats <span style="color:#f92672">import</span> norm
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Install additional dependencies</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># !pip install scikit-optimize==0.5.2</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># !pip install GPy==1.9.8</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># !pip install GPyOpt==1.2.1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># !pip install xgboost==0.90</span>
</span></span></code></pre></div><h1 id="objective-function">Objective Function</h1>
<p>We of course need a function to optimize so let&rsquo;s create one. We also define the bounds and the inital sample points for our optimization problem.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>bounds <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">4.0</span>]])
</span></span><span style="display:flex;"><span>noise <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">f</span>(X, noise<span style="color:#f92672">=</span>noise):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>sin(<span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>X) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>cos(X) <span style="color:#f92672">+</span> noise <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(<span style="color:#f92672">*</span>X<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X_init <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">1.5</span>], [<span style="color:#ae81ff">2</span>]])
</span></span><span style="display:flex;"><span>Y_init <span style="color:#f92672">=</span> f(X_init)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Dense grid of points within bounds</span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(bounds[:, <span style="color:#ae81ff">0</span>], bounds[:, <span style="color:#ae81ff">1</span>], <span style="color:#ae81ff">0.01</span>)<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Noise-free objective function values at X </span>
</span></span><span style="display:flex;"><span>Y <span style="color:#f92672">=</span> f(X,<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot optimization objective with noise level </span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(X, Y, <span style="color:#e6db74">&#39;m--&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Noise-free objective&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(X, f(X), <span style="color:#e6db74">&#39;b.&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Noisy samples&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(X_init, Y_init, <span style="color:#e6db74">&#39;ko&#39;</span>, mew<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Initial samples&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend();
</span></span></code></pre></div><p><img alt="png" src="../../images/bayes-opt/bayes-opt_8_0.png"></p>
<h1 id="plotting-function">Plotting Function</h1>
<p>We will want to plot our progress later on so let&rsquo;s equipped ourself with some plotting methods.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_approximation</span>(gpr, X, Y, X_sample, Y_sample, X_next<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, show_legend<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>    mu, std <span style="color:#f92672">=</span> gpr<span style="color:#f92672">.</span>predict(X, return_std<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>fill_between(X<span style="color:#f92672">.</span>ravel(), 
</span></span><span style="display:flex;"><span>                     mu<span style="color:#f92672">.</span>ravel() <span style="color:#f92672">+</span> <span style="color:#ae81ff">1.96</span> <span style="color:#f92672">*</span> std, 
</span></span><span style="display:flex;"><span>                     mu<span style="color:#f92672">.</span>ravel() <span style="color:#f92672">-</span> <span style="color:#ae81ff">1.96</span> <span style="color:#f92672">*</span> std, 
</span></span><span style="display:flex;"><span>                     alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>) 
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(X, Y, <span style="color:#e6db74">&#39;m--&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Noise-free objective&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(X, mu, <span style="color:#e6db74">&#39;b-&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Surrogate function&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(X_sample, Y_sample, <span style="color:#e6db74">&#39;kx&#39;</span>, mew<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Noisy samples&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> X_next:
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>axvline(x<span style="color:#f92672">=</span>X_next, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> show_legend:
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>legend()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_acquisition</span>(X, Y, X_next, show_legend<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(X, Y, <span style="color:#e6db74">&#39;r-&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Acquisition function&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>axvline(x<span style="color:#f92672">=</span>X_next, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k&#39;</span>, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Next sampling location&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> show_legend:
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>legend()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_convergence</span>(X_sample, Y_sample, n_init<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> X_sample[n_init:]<span style="color:#f92672">.</span>ravel()
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> Y_sample[n_init:]<span style="color:#f92672">.</span>ravel()
</span></span><span style="display:flex;"><span>    r <span style="color:#f92672">=</span> range(<span style="color:#ae81ff">1</span>, len(x)<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    x_neighbor_dist <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>abs(a<span style="color:#f92672">-</span>b) <span style="color:#66d9ef">for</span> a, b <span style="color:#f92672">in</span> zip(x, x[<span style="color:#ae81ff">1</span>:])]
</span></span><span style="display:flex;"><span>    y_max_watermark <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>maximum<span style="color:#f92672">.</span>accumulate(y)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(r[<span style="color:#ae81ff">1</span>:], x_neighbor_dist, <span style="color:#e6db74">&#39;bo-&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Iteration&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Distance&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Distance between consecutive x</span><span style="color:#ae81ff">\&#39;</span><span style="color:#e6db74">s&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(r, y_max_watermark, <span style="color:#e6db74">&#39;ro-&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Iteration&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Best Y&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Value of best selected sample&#39;</span>)
</span></span></code></pre></div><p>Our overall goal is to find the global optimum on the left in a small number of steps. To do this, we need to implement the acquisition function defined in Equation (2) as expected_improvement function.</p>
<p>Activation function is the fucntion that determines which point we should evalulate next</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">expected_improvement</span>(X, X_sample, Y_sample, gpr, xi<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Computes the EI at points X based on existing samples X_sample
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    and Y_sample using a Gaussian process surrogate model.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        X: Points at which EI shall be computed (m x d).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        X_sample: Sample locations (n x d).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Y_sample: Sample values (n x 1).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        gpr: A GaussianProcessRegressor fitted to samples.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        xi: Exploitation-exploration trade-off parameter.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Expected improvements at points X.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    mu, sigma <span style="color:#f92672">=</span> gpr<span style="color:#f92672">.</span>predict(X, return_std<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    mu_sample <span style="color:#f92672">=</span> gpr<span style="color:#f92672">.</span>predict(X_sample)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    sigma <span style="color:#f92672">=</span> sigma<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Needed for noise-based model,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># otherwise use np.max(Y_sample).</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># See also section 2.4 in [...]</span>
</span></span><span style="display:flex;"><span>    mu_sample_opt <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(mu_sample)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> np<span style="color:#f92672">.</span>errstate(divide<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;warn&#39;</span>):
</span></span><span style="display:flex;"><span>        imp <span style="color:#f92672">=</span> mu <span style="color:#f92672">-</span> mu_sample_opt <span style="color:#f92672">-</span> xi
</span></span><span style="display:flex;"><span>        Z <span style="color:#f92672">=</span> imp <span style="color:#f92672">/</span> sigma
</span></span><span style="display:flex;"><span>        ei <span style="color:#f92672">=</span> imp <span style="color:#f92672">*</span> norm<span style="color:#f92672">.</span>cdf(Z) <span style="color:#f92672">+</span> sigma <span style="color:#f92672">*</span> norm<span style="color:#f92672">.</span>pdf(Z)
</span></span><span style="display:flex;"><span>        ei[sigma <span style="color:#f92672">==</span> <span style="color:#ae81ff">0.0</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> ei
</span></span></code></pre></div><p>We also need a function that proposes the next sampling point by computing the location of the acquisition function maximum. Optimization is restarted n_restarts times to avoid local optima.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">propose_location</span>(acquisition, X_sample, Y_sample, gpr, bounds, n_restarts<span style="color:#f92672">=</span><span style="color:#ae81ff">25</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Proposes the next sampling point by optimizing the acquisition function.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        acquisition: Acquisition function.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        X_sample: Sample locations (n x d).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Y_sample: Sample values (n x 1).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        gpr: A GaussianProcessRegressor fitted to samples.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Location of the acquisition function maximum.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    dim <span style="color:#f92672">=</span> X_sample<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    min_val <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    min_x <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">min_obj</span>(X):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Minimization objective is the negative acquisition function</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>acquisition(X<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, dim), X_sample, Y_sample, gpr)<span style="color:#f92672">.</span>ravel()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Find the best optimum by starting from n_restart different random points.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> x0 <span style="color:#f92672">in</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(bounds[:, <span style="color:#ae81ff">0</span>], bounds[:, <span style="color:#ae81ff">1</span>], size<span style="color:#f92672">=</span>(n_restarts, dim)):
</span></span><span style="display:flex;"><span>        res <span style="color:#f92672">=</span> minimize(min_obj, x0<span style="color:#f92672">=</span>x0, bounds<span style="color:#f92672">=</span>bounds, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;L-BFGS-B&#39;</span>)        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> res<span style="color:#f92672">.</span>fun <span style="color:#f92672">&lt;</span> min_val:
</span></span><span style="display:flex;"><span>            min_val <span style="color:#f92672">=</span> res<span style="color:#f92672">.</span>fun[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>            min_x <span style="color:#f92672">=</span> res<span style="color:#f92672">.</span>x           
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> min_x<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><p>Now we have all components needed to run Bayesian optimization with the algorithm outlined above.</p>
<p>We have plotted the objective function, the GP posterior predictive mean, the 95% confidence interval of the mean and the samples obtained from the objective function so far with noise. The right plot shows the acquisition function. The maximum of the aquisition function is what&rsquo;s used to propose the next sampling point for the next iteration&mdash;denoted by a black vertical dashed line.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Gaussian process with Matérn kernel as surrogate model</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The Gaussian process in the following example is configured with a Matérn kernel</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># generalization of the squared exponential kernel or RBF kernel</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Alpha parameters configures the known noise level</span>
</span></span><span style="display:flex;"><span>m52 <span style="color:#f92672">=</span> ConstantKernel(<span style="color:#ae81ff">1.0</span>) <span style="color:#f92672">*</span> Matern(length_scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, nu<span style="color:#f92672">=</span><span style="color:#ae81ff">2.5</span>)
</span></span><span style="display:flex;"><span>gpr <span style="color:#f92672">=</span> GaussianProcessRegressor(kernel<span style="color:#f92672">=</span>m52, alpha<span style="color:#f92672">=</span>noise<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize samples</span>
</span></span><span style="display:flex;"><span>X_sample <span style="color:#f92672">=</span> X_init
</span></span><span style="display:flex;"><span>Y_sample <span style="color:#f92672">=</span> Y_init
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Number of iterations</span>
</span></span><span style="display:flex;"><span>n_iter <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, n_iter <span style="color:#f92672">*</span> <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplots_adjust(hspace<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n_iter):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Update Gaussian process with existing samples</span>
</span></span><span style="display:flex;"><span>    gpr<span style="color:#f92672">.</span>fit(X_sample, Y_sample)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Obtain next sampling point from the acquisition function (expected_improvement)</span>
</span></span><span style="display:flex;"><span>    X_next <span style="color:#f92672">=</span> propose_location(expected_improvement, X_sample, Y_sample, gpr, bounds)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Obtain next noisy sample from the objective function</span>
</span></span><span style="display:flex;"><span>    Y_next <span style="color:#f92672">=</span> f(X_next, noise)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Plot samples, surrogate function, noise-free objective and next sampling location</span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>subplot(n_iter, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> i <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    plot_approximation(gpr, X, Y, X_sample, Y_sample, X_next, show_legend<span style="color:#f92672">=</span>i<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Iteration </span><span style="color:#e6db74">{</span>i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>subplot(n_iter, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> i <span style="color:#f92672">+</span> <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    plot_acquisition(X, expected_improvement(X, X_sample, Y_sample, gpr), X_next, show_legend<span style="color:#f92672">=</span>i<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Add sample to previous samples</span>
</span></span><span style="display:flex;"><span>    X_sample <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>vstack((X_sample, X_next))
</span></span><span style="display:flex;"><span>    Y_sample <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>vstack((Y_sample, Y_next))
</span></span></code></pre></div><p><img alt="png" src="../../images/bayes-opt/bayes-opt_19_0.png"></p>
<h1 id="using-python-libraries">Using Python Libraries</h1>
<p>Scikit-optimize</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.base <span style="color:#f92672">import</span> clone
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> skopt <span style="color:#f92672">import</span> gp_minimize
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> skopt.learning <span style="color:#f92672">import</span> GaussianProcessRegressor
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> skopt.learning.gaussian_process.kernels <span style="color:#f92672">import</span> ConstantKernel, Matern
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Use custom kernel and estimator to match previous example</span>
</span></span><span style="display:flex;"><span>m52 <span style="color:#f92672">=</span> ConstantKernel(<span style="color:#ae81ff">1.0</span>) <span style="color:#f92672">*</span> Matern(length_scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, nu<span style="color:#f92672">=</span><span style="color:#ae81ff">2.5</span>)
</span></span><span style="display:flex;"><span>gpr <span style="color:#f92672">=</span> GaussianProcessRegressor(kernel<span style="color:#f92672">=</span>m52, alpha<span style="color:#f92672">=</span>noise<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>r <span style="color:#f92672">=</span> gp_minimize(<span style="color:#66d9ef">lambda</span> x: <span style="color:#f92672">-</span>f(np<span style="color:#f92672">.</span>array(x))[<span style="color:#ae81ff">0</span>], 
</span></span><span style="display:flex;"><span>                bounds<span style="color:#f92672">.</span>tolist(),
</span></span><span style="display:flex;"><span>                base_estimator<span style="color:#f92672">=</span>gpr,
</span></span><span style="display:flex;"><span>                acq_func<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;EI&#39;</span>,      <span style="color:#75715e"># expected improvement</span>
</span></span><span style="display:flex;"><span>                xi<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>,            <span style="color:#75715e"># exploitation-exploration trade-off</span>
</span></span><span style="display:flex;"><span>                n_calls<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,         <span style="color:#75715e"># number of iterations</span>
</span></span><span style="display:flex;"><span>                n_random_starts<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,  <span style="color:#75715e"># initial samples are provided</span>
</span></span><span style="display:flex;"><span>                x0<span style="color:#f92672">=</span>X_init<span style="color:#f92672">.</span>tolist(), <span style="color:#75715e"># initial samples</span>
</span></span><span style="display:flex;"><span>                y0<span style="color:#f92672">=-</span>Y_init<span style="color:#f92672">.</span>ravel())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Fit GP model to samples for plotting results</span>
</span></span><span style="display:flex;"><span>gpr<span style="color:#f92672">.</span>fit(r<span style="color:#f92672">.</span>x_iters, <span style="color:#f92672">-</span>r<span style="color:#f92672">.</span>func_vals)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the fitted model and the noisy samples</span>
</span></span><span style="display:flex;"><span>plot_approximation(gpr, X, Y, r<span style="color:#f92672">.</span>x_iters, <span style="color:#f92672">-</span>r<span style="color:#f92672">.</span>func_vals, show_legend<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p><img alt="png" src="../../images/bayes-opt/bayes-opt_22_0.png"></p>
<p>There are numerous Bayesian optimization libraries out there and giving a comprehensive overview is not the goal of this article. Instead, I&rsquo;ll pick two that I used in the past and show the minimum setup needed to get the previous example running.</p>
<p>Scikit-optimize is a library for sequential model-based optimization that is based on scikit-learn. It also supports Bayesian optimization using Gaussian processes. The API is designed around minimization, hence, we have to provide negative objective function values. The results obtained here slightly differ from previous results because of non-deterministic optimization behavior and different noisy samples drawn from the objective function.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> GPy
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> GPyOpt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> GPyOpt.methods <span style="color:#f92672">import</span> BayesianOptimization
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>kernel <span style="color:#f92672">=</span> GPy<span style="color:#f92672">.</span>kern<span style="color:#f92672">.</span>Matern52(input_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, variance<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, lengthscale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>bds <span style="color:#f92672">=</span> [{<span style="color:#e6db74">&#39;name&#39;</span>: <span style="color:#e6db74">&#39;X&#39;</span>, <span style="color:#e6db74">&#39;type&#39;</span>: <span style="color:#e6db74">&#39;continuous&#39;</span>, <span style="color:#e6db74">&#39;domain&#39;</span>: bounds<span style="color:#f92672">.</span>ravel()}]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> BayesianOptimization(f<span style="color:#f92672">=</span>f, 
</span></span><span style="display:flex;"><span>                                 domain<span style="color:#f92672">=</span>bds,
</span></span><span style="display:flex;"><span>                                 model_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;GP&#39;</span>,
</span></span><span style="display:flex;"><span>                                 kernel<span style="color:#f92672">=</span>kernel,
</span></span><span style="display:flex;"><span>                                 acquisition_type <span style="color:#f92672">=</span><span style="color:#e6db74">&#39;EI&#39;</span>,
</span></span><span style="display:flex;"><span>                                 acquisition_jitter <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>,
</span></span><span style="display:flex;"><span>                                 X<span style="color:#f92672">=</span>X_init,
</span></span><span style="display:flex;"><span>                                 Y<span style="color:#f92672">=-</span>Y_init,
</span></span><span style="display:flex;"><span>                                 noise_var <span style="color:#f92672">=</span> noise<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>,
</span></span><span style="display:flex;"><span>                                 exact_feval<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>                                 normalize_Y<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>                                 maximize<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>optimizer<span style="color:#f92672">.</span>run_optimization(max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">40</span>)
</span></span><span style="display:flex;"><span>optimizer<span style="color:#f92672">.</span>plot_acquisition()
</span></span></code></pre></div><p><img alt="png" src="../../images/bayes-opt/bayes-opt_24_0.png"></p>

		</div>
	</section>
</article>
				
					<footer id="footer">
						<ul class="icons">
              
              <li><a href="https://www.linkedin.com/in/eric-pena" class="icon brands fa-linkedin" target="_blank" rel="noopener noreferrer"><span class="label">LinkedIn</span></a></li>
              
              
              <li><a href="https://twitter.com/ericpenax" class="icon brands fa-twitter" target="_blank" rel="noopener noreferrer"><span class="label">Twitter</span></a></li>
              
              
              <li><a href="https://mstdn.science/@ericpena" class="icon brands fa-mastodon" target="_blank" rel="noopener noreferrer"><span class="label">Mastodon</span></a></li>
              
              
              <li><a href="https://github.com/ericpena" class="icon brands fa-github" target="_blank" rel="noopener noreferrer"><span class="label">GitHub</span></a></li>
              
              
              
              
              <li><a href="https://instagram.com/ericpena" class="icon brands fa-instagram" target="_blank" rel="noopener noreferrer"><span class="label">Instagram</span></a></li>
              
              
              
              <li><a href="mailto:eric.pena@binghamton.edu" class="icon solid fa-envelope" target="_blank" rel="noopener noreferrer"><span class="label">Email</span></a></li>
              
              

						</ul>
						<ul class="copyright">
              <li>&copy; 2025 Eric Peña</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</footer>

			</div>

      









<script src="../../js/bundle.min.935254271ae3006602cb92b38ba70062a462cefc8d3aa575338369d256bb8422a69fc18f64450e74b7e6c240b20a252f522f3b9f323294bdf9ed466f5fb28ee9.js" integrity="sha512-k1JUJxrjAGYCy5Kzi6cAYqRizvyNOqV1M4Np0la7hCKmn8GPZEUOdLfmwkCyCiUvUi87nzIylL357UZvX7KO6Q=="></script>


	</body>
</html>

